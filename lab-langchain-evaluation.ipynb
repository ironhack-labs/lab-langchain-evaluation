{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b00dae2-c271-4a69-a579-742e084a9058",
   "metadata": {},
   "source": [
    "# Lab | Langchain Evaluation\n",
    "\n",
    "## Intro\n",
    "\n",
    "Pick different sets of data and re-run this notebook. The point is for you to understand all steps involve and the many different ways one can and should evaluate LLM applications.\n",
    "\n",
    "What did you learn? - Let's discuss that in class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52824b89-532a-4e54-87e9-1410813cd39e",
   "metadata": {},
   "source": [
    "## LangChain: Evaluation\n",
    "\n",
    "### Outline:\n",
    "\n",
    "* Example generation\n",
    "* Manual evaluation (and debuging)\n",
    "* LLM-assisted evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Retrieve the OpenAI API key from environment variables\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check if the API key was loaded successfully\n",
    "if OPENAI_API_KEY is None:\n",
    "    raise ValueError(\"Error: OPENAI_API_KEY not found in environment variables.\")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e29d2c-ba67-4cba-8ded-375fe040b9ba",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28008949",
   "metadata": {},
   "source": [
    "#### Create our QandA application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71a7912b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from langchain-huggingface) (0.26.1)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from langchain-huggingface) (0.3.15)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from langchain-huggingface) (3.2.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from langchain-huggingface) (0.20.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from langchain-huggingface) (4.45.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.1.136)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (9.0.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.5.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.14.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.10.9)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2024.8.30)\n",
      "Requirement already satisfied: networkx in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain-huggingface) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mktmi\\anaconda3\\envs\\projetofinal\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.5)\n",
      "Downloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: langchain-huggingface\n",
      "Successfully installed langchain-huggingface-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "974acf8e-8f88-42de-88f8-40a82cb58e8b",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import CSVLoader, TextLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.chains import LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ec1106d",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records loaded: 1000\n",
      "\n",
      "Record 1:\n",
      "Page Content:\n",
      ": 0\n",
      "name: Women's Campside Oxfords\n",
      "description: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \n",
      "\n",
      "Size & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \n",
      "\n",
      "Specs: Approx. weight: 1 lb.1 oz. per pair. \n",
      "\n",
      "Construction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT® antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \n",
      "\n",
      "Questions? Please contact us for any inquiries.\n",
      "Metadata:\n",
      "{'source': 'data\\\\OutdoorClothingCatalog_1000.csv', 'row': 0}\n",
      "\n",
      "Record 2:\n",
      "Page Content:\n",
      ": 1\n",
      "name: Recycled Waterhog Dog Mat, Chevron Weave\n",
      "description: Protect your floors from spills and splashing with our ultradurable recycled Waterhog dog mat made right here in the USA. \n",
      "\n",
      "Specs\n",
      "Small - Dimensions: 18\" x 28\". \n",
      "Medium - Dimensions: 22.5\" x 34.5\".\n",
      "\n",
      "Why We Love It\n",
      "Mother nature, wet shoes and muddy paws have met their match with our Recycled Waterhog mats. Ruggedly constructed from recycled plastic materials, these ultratough mats help keep dirt and water off your floors and plastic out of landfills, trails and oceans. Now, that's a win-win for everyone.\n",
      "\n",
      "Fabric & Care\n",
      "Vacuum or hose clean.\n",
      "\n",
      "Construction\n",
      "24 oz. polyester fabric made from 94% recycled materials.\n",
      "Rubber backing.\n",
      "\n",
      "Additional Features\n",
      "Features an -exclusive design.\n",
      "Features thick and thin fibers for scraping dirt and absorbing water.\n",
      "Dries quickly and resists fading, rotting, mildew and shedding.\n",
      "Use indoors or out.\n",
      "Made in the USA.\n",
      "\n",
      "Have questions? Reach out to our customer service team with any questions you may have.\n",
      "Metadata:\n",
      "{'source': 'data\\\\OutdoorClothingCatalog_1000.csv', 'row': 1}\n",
      "\n",
      "Record 3:\n",
      "Page Content:\n",
      ": 2\n",
      "name: Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece\n",
      "description: She'll love the bright colors, ruffles and exclusive whimsical prints of this toddler's two-piece swimsuit! Our four-way-stretch and chlorine-resistant fabric keeps its shape and resists snags. The UPF 50+ rated fabric provides the highest rated sun protection possible, blocking 98% of the sun's harmful rays. The crossover no-slip straps and fully lined bottom ensure a secure fit and maximum coverage. Machine wash and line dry for best results. Imported.\n",
      "Metadata:\n",
      "{'source': 'data\\\\OutdoorClothingCatalog_1000.csv', 'row': 2}\n",
      "\n",
      "Record 4:\n",
      "Page Content:\n",
      ": 3\n",
      "name: Refresh Swimwear, V-Neck Tankini Contrasts\n",
      "description: Whether you're going for a swim or heading out on an SUP, this watersport-ready tankini top is designed to move with you and stay comfortable. All while looking great in an eye-catching colorblock style. \n",
      "\n",
      "Size & Fit\n",
      "Fitted: Sits close to the body.\n",
      "\n",
      "Why We Love It\n",
      "Not only does this swimtop feel good to wear, its fabric is good for the earth too. In recycled nylon, with Lycra® spandex for the perfect amount of stretch. \n",
      "\n",
      "Fabric & Care\n",
      "The premium Italian-blend is breathable, quick drying and abrasion resistant. \n",
      "Body in 82% recycled nylon with 18% Lycra® spandex. \n",
      "Lined in 90% recycled nylon with 10% Lycra® spandex. \n",
      "UPF 50+ rated – the highest rated sun protection possible. \n",
      "Handwash, line dry.\n",
      "\n",
      "Additional Features\n",
      "Lightweight racerback straps are easy to get on and off, and won't get in your way. \n",
      "Flattering V-neck silhouette. \n",
      "Imported.\n",
      "\n",
      "Sun Protection That Won't Wear Off\n",
      "Our high-performance fabric provides SPF\n",
      "Metadata:\n",
      "{'source': 'data\\\\OutdoorClothingCatalog_1000.csv', 'row': 3}\n",
      "\n",
      "Record 5:\n",
      "Page Content:\n",
      ": 4\n",
      "name: EcoFlex 3L Storm Pants\n",
      "description: Our new TEK O2 technology makes our four-season waterproof pants even more breathable. It's guaranteed to keep you dry and comfortable – whatever the activity and whatever the weather. Size & Fit: Slightly Fitted through hip and thigh. \n",
      "\n",
      "Why We Love It: Our state-of-the-art TEK O2 technology offers the most breathability we've ever tested. Great as ski pants, they're ideal for a variety of outdoor activities year-round. Plus, they're loaded with features outdoor enthusiasts appreciate, including weather-blocking gaiters and handy side zips. Air In. Water Out. See how our air-permeable TEK O2 technology keeps you dry and comfortable. \n",
      "\n",
      "Fabric & Care: 100% nylon, exclusive of trim. Machine wash and dry. \n",
      "\n",
      "Additional Features: Three-layer shell delivers waterproof protection. Brand new TEK O2 technology provides enhanced breathability. Interior gaiters keep out rain and snow. Full side zips for easy on/off over boots. Two zippered hand pockets. Thigh pocket. Imported.\n",
      "\n",
      " – Official Supplier to the U.S. Ski Team\n",
      "THEIR WILL\n",
      "Metadata:\n",
      "{'source': 'data\\\\OutdoorClothingCatalog_1000.csv', 'row': 4}\n"
     ]
    }
   ],
   "source": [
    "file = 'data\\OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file)\n",
    "data = loader.load()\n",
    "\n",
    "# Verify the number of records loaded\n",
    "print(f\"Number of records loaded: {len(data)}\")\n",
    "\n",
    "# Inspect the structure of the first few records\n",
    "for i, record in enumerate(data[:5]):\n",
    "    print(f\"\\nRecord {i+1}:\")\n",
    "    print(f\"Page Content:\\n{record.page_content}\")\n",
    "    print(f\"Metadata:\\n{record.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "550eb642-c223-4d78-8f92-0f265ef78b86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Using cached tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.5.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.5.2-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.14.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Using cached huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Using cached pillow-11.0.0-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting packaging>=20.9 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting requests (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting colorama (from tqdm->sentence-transformers)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting numpy>=1.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading numpy-2.1.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.4.5-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.20.3-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached charset_normalizer-3.4.0-cp311-cp311-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
      "Using cached huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading torch-2.5.1-cp311-cp311-win_amd64.whl (203.1 MB)\n",
      "   ---------------------------------------- 0.0/203.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/203.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/203.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/203.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/203.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/203.1 MB 622.7 kB/s eta 0:05:26\n",
      "   ---------------------------------------- 0.5/203.1 MB 622.7 kB/s eta 0:05:26\n",
      "   ---------------------------------------- 0.8/203.1 MB 559.5 kB/s eta 0:06:02\n",
      "   ---------------------------------------- 0.8/203.1 MB 559.5 kB/s eta 0:06:02\n",
      "   ---------------------------------------- 1.0/203.1 MB 592.2 kB/s eta 0:05:42\n",
      "   ---------------------------------------- 1.3/203.1 MB 664.5 kB/s eta 0:05:04\n",
      "   ---------------------------------------- 1.8/203.1 MB 883.1 kB/s eta 0:03:48\n",
      "    --------------------------------------- 2.6/203.1 MB 1.2 MB/s eta 0:02:50\n",
      "    --------------------------------------- 3.4/203.1 MB 1.4 MB/s eta 0:02:19\n",
      "    --------------------------------------- 4.5/203.1 MB 1.7 MB/s eta 0:01:55\n",
      "    --------------------------------------- 5.0/203.1 MB 1.8 MB/s eta 0:01:49\n",
      "   - -------------------------------------- 5.5/203.1 MB 1.9 MB/s eta 0:01:45\n",
      "   - -------------------------------------- 6.3/203.1 MB 2.0 MB/s eta 0:01:39\n",
      "   - -------------------------------------- 7.3/203.1 MB 2.2 MB/s eta 0:01:31\n",
      "   - -------------------------------------- 8.4/203.1 MB 2.3 MB/s eta 0:01:24\n",
      "   - -------------------------------------- 8.9/203.1 MB 2.4 MB/s eta 0:01:23\n",
      "   -- ------------------------------------- 10.2/203.1 MB 2.6 MB/s eta 0:01:16\n",
      "   -- ------------------------------------- 11.3/203.1 MB 2.7 MB/s eta 0:01:11\n",
      "   -- ------------------------------------- 12.8/203.1 MB 2.9 MB/s eta 0:01:06\n",
      "   -- ------------------------------------- 14.2/203.1 MB 3.1 MB/s eta 0:01:02\n",
      "   --- ------------------------------------ 15.5/203.1 MB 3.2 MB/s eta 0:00:59\n",
      "   --- ------------------------------------ 16.8/203.1 MB 3.3 MB/s eta 0:00:56\n",
      "   --- ------------------------------------ 17.8/203.1 MB 3.4 MB/s eta 0:00:55\n",
      "   --- ------------------------------------ 19.1/203.1 MB 3.5 MB/s eta 0:00:53\n",
      "   ---- ----------------------------------- 20.4/203.1 MB 3.6 MB/s eta 0:00:51\n",
      "   ---- ----------------------------------- 21.2/203.1 MB 3.7 MB/s eta 0:00:50\n",
      "   ---- ----------------------------------- 22.5/203.1 MB 3.7 MB/s eta 0:00:49\n",
      "   ---- ----------------------------------- 23.9/203.1 MB 3.8 MB/s eta 0:00:47\n",
      "   ---- ----------------------------------- 25.2/203.1 MB 3.9 MB/s eta 0:00:46\n",
      "   ----- ---------------------------------- 26.7/203.1 MB 4.0 MB/s eta 0:00:44\n",
      "   ----- ---------------------------------- 28.0/203.1 MB 4.1 MB/s eta 0:00:43\n",
      "   ----- ---------------------------------- 29.6/203.1 MB 4.2 MB/s eta 0:00:42\n",
      "   ------ --------------------------------- 31.2/203.1 MB 4.3 MB/s eta 0:00:41\n",
      "   ------ --------------------------------- 32.5/203.1 MB 4.3 MB/s eta 0:00:40\n",
      "   ------ --------------------------------- 33.6/203.1 MB 4.4 MB/s eta 0:00:39\n",
      "   ------ --------------------------------- 35.1/203.1 MB 4.5 MB/s eta 0:00:38\n",
      "   ------- -------------------------------- 36.7/203.1 MB 4.5 MB/s eta 0:00:37\n",
      "   ------- -------------------------------- 38.3/203.1 MB 4.6 MB/s eta 0:00:36\n",
      "   ------- -------------------------------- 39.6/203.1 MB 4.7 MB/s eta 0:00:36\n",
      "   -------- ------------------------------- 41.2/203.1 MB 4.7 MB/s eta 0:00:35\n",
      "   -------- ------------------------------- 42.5/203.1 MB 4.8 MB/s eta 0:00:34\n",
      "   -------- ------------------------------- 44.0/203.1 MB 4.8 MB/s eta 0:00:33\n",
      "   -------- ------------------------------- 45.6/203.1 MB 4.9 MB/s eta 0:00:33\n",
      "   --------- ------------------------------ 46.9/203.1 MB 4.9 MB/s eta 0:00:32\n",
      "   --------- ------------------------------ 48.2/203.1 MB 4.9 MB/s eta 0:00:32\n",
      "   --------- ------------------------------ 49.5/203.1 MB 5.0 MB/s eta 0:00:31\n",
      "   ---------- ----------------------------- 51.1/203.1 MB 5.0 MB/s eta 0:00:31\n",
      "   ---------- ----------------------------- 52.4/203.1 MB 5.1 MB/s eta 0:00:30\n",
      "   ---------- ----------------------------- 54.3/203.1 MB 5.1 MB/s eta 0:00:30\n",
      "   ---------- ----------------------------- 55.8/203.1 MB 5.2 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 56.9/203.1 MB 5.2 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 58.2/203.1 MB 5.2 MB/s eta 0:00:28\n",
      "   ----------- ---------------------------- 59.5/203.1 MB 5.2 MB/s eta 0:00:28\n",
      "   ----------- ---------------------------- 60.8/203.1 MB 5.2 MB/s eta 0:00:28\n",
      "   ------------ --------------------------- 62.1/203.1 MB 5.3 MB/s eta 0:00:27\n",
      "   ------------ --------------------------- 63.7/203.1 MB 5.3 MB/s eta 0:00:27\n",
      "   ------------ --------------------------- 65.3/203.1 MB 5.3 MB/s eta 0:00:26\n",
      "   ------------- -------------------------- 66.8/203.1 MB 5.4 MB/s eta 0:00:26\n",
      "   ------------- -------------------------- 68.4/203.1 MB 5.4 MB/s eta 0:00:25\n",
      "   ------------- -------------------------- 70.0/203.1 MB 5.4 MB/s eta 0:00:25\n",
      "   -------------- ------------------------- 71.3/203.1 MB 5.5 MB/s eta 0:00:25\n",
      "   -------------- ------------------------- 72.6/203.1 MB 5.5 MB/s eta 0:00:24\n",
      "   -------------- ------------------------- 74.2/203.1 MB 5.5 MB/s eta 0:00:24\n",
      "   --------------- ------------------------ 76.3/203.1 MB 5.6 MB/s eta 0:00:23\n",
      "   --------------- ------------------------ 77.9/203.1 MB 5.6 MB/s eta 0:00:23\n",
      "   --------------- ------------------------ 79.7/203.1 MB 5.6 MB/s eta 0:00:22\n",
      "   ---------------- ----------------------- 81.3/203.1 MB 5.7 MB/s eta 0:00:22\n",
      "   ---------------- ----------------------- 82.8/203.1 MB 5.7 MB/s eta 0:00:22\n",
      "   ---------------- ----------------------- 84.7/203.1 MB 5.7 MB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 86.2/203.1 MB 5.8 MB/s eta 0:00:21\n",
      "   ----------------- ---------------------- 87.8/203.1 MB 5.8 MB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 89.4/203.1 MB 5.8 MB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 91.0/203.1 MB 5.8 MB/s eta 0:00:20\n",
      "   ------------------ --------------------- 92.5/203.1 MB 5.8 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 94.1/203.1 MB 5.9 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 95.7/203.1 MB 5.9 MB/s eta 0:00:19\n",
      "   ------------------- -------------------- 97.5/203.1 MB 5.9 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 99.1/203.1 MB 5.9 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 100.7/203.1 MB 6.0 MB/s eta 0:00:18\n",
      "   -------------------- ------------------- 102.2/203.1 MB 6.0 MB/s eta 0:00:17\n",
      "   -------------------- ------------------- 104.1/203.1 MB 6.0 MB/s eta 0:00:17\n",
      "   -------------------- ------------------- 105.6/203.1 MB 6.0 MB/s eta 0:00:17\n",
      "   --------------------- ------------------ 107.0/203.1 MB 6.0 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 108.8/203.1 MB 6.1 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 110.4/203.1 MB 6.1 MB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 111.9/203.1 MB 6.1 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 113.5/203.1 MB 6.1 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 115.1/203.1 MB 6.1 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 116.7/203.1 MB 6.2 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 118.2/203.1 MB 6.2 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 119.8/203.1 MB 6.2 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 121.4/203.1 MB 6.2 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 122.9/203.1 MB 6.2 MB/s eta 0:00:13\n",
      "   ------------------------ --------------- 124.5/203.1 MB 6.2 MB/s eta 0:00:13\n",
      "   ------------------------ --------------- 126.1/203.1 MB 6.2 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 127.7/203.1 MB 6.3 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 129.5/203.1 MB 6.3 MB/s eta 0:00:12\n",
      "   ------------------------- -------------- 130.8/203.1 MB 6.3 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 132.4/203.1 MB 6.3 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 134.2/203.1 MB 6.3 MB/s eta 0:00:11\n",
      "   -------------------------- ------------- 135.8/203.1 MB 6.3 MB/s eta 0:00:11\n",
      "   --------------------------- ------------ 137.4/203.1 MB 6.3 MB/s eta 0:00:11\n",
      "   --------------------------- ------------ 138.9/203.1 MB 6.4 MB/s eta 0:00:11\n",
      "   --------------------------- ------------ 140.5/203.1 MB 6.4 MB/s eta 0:00:10\n",
      "   --------------------------- ------------ 142.1/203.1 MB 6.4 MB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 143.7/203.1 MB 6.4 MB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 145.2/203.1 MB 6.4 MB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 146.8/203.1 MB 6.4 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 148.4/203.1 MB 6.4 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 149.9/203.1 MB 6.4 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 151.5/203.1 MB 6.4 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 153.1/203.1 MB 6.5 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 154.7/203.1 MB 6.5 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 156.2/203.1 MB 6.5 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 157.8/203.1 MB 6.5 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 159.4/203.1 MB 6.5 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 161.0/203.1 MB 6.5 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 162.5/203.1 MB 6.5 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 164.1/203.1 MB 6.5 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 165.4/203.1 MB 6.5 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 167.0/203.1 MB 6.5 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 168.6/203.1 MB 6.5 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 170.1/203.1 MB 6.6 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 171.7/203.1 MB 6.6 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 173.3/203.1 MB 6.6 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 174.9/203.1 MB 6.6 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 176.4/203.1 MB 6.6 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 178.0/203.1 MB 6.6 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 179.8/203.1 MB 6.6 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 181.4/203.1 MB 6.6 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 183.0/203.1 MB 6.6 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 184.8/203.1 MB 6.6 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 186.4/203.1 MB 6.6 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 188.0/203.1 MB 6.7 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 189.5/203.1 MB 6.7 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 191.1/203.1 MB 6.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 192.7/203.1 MB 6.7 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 194.2/203.1 MB 6.7 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 195.8/203.1 MB 6.7 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 197.4/203.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  199.0/203.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  200.5/203.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.1/203.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 203.1/203.1 MB 6.8 MB/s eta 0:00:00\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "Using cached pillow-11.0.0-cp311-cp311-win_amd64.whl (2.6 MB)\n",
      "Using cached scikit_learn-1.5.2-cp311-cp311-win_amd64.whl (11.0 MB)\n",
      "Using cached scipy-1.14.1-cp311-cp311-win_amd64.whl (44.8 MB)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading numpy-2.1.3-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/12.9 MB 3.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.9/12.9 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.2/12.9 MB 7.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 6.0/12.9 MB 7.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.6/12.9 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.2/12.9 MB 7.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.5/12.9 MB 7.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.7/12.9 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.0/12.9 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.8/12.9 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.8/12.9 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.8/12.9 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.9 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 4.4 MB/s eta 0:00:00\n",
      "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "Using cached safetensors-0.4.5-cp311-none-win_amd64.whl (285 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.20.3-cp311-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 1.3/2.4 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 7.1 MB/s eta 0:00:00\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp311-cp311-win_amd64.whl (101 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: mpmath, urllib3, typing-extensions, threadpoolctl, sympy, safetensors, regex, pyyaml, Pillow, packaging, numpy, networkx, MarkupSafe, joblib, idna, fsspec, filelock, colorama, charset-normalizer, certifi, tqdm, scipy, requests, jinja2, torch, scikit-learn, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.3.0\n",
      "    Uninstalling mpmath-1.3.0:\n",
      "      Successfully uninstalled mpmath-1.3.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: threadpoolctl\n",
      "    Found existing installation: threadpoolctl 3.5.0\n",
      "    Uninstalling threadpoolctl-3.5.0:\n",
      "      Successfully uninstalled threadpoolctl-3.5.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.5\n",
      "    Uninstalling safetensors-0.4.5:\n",
      "      Successfully uninstalled safetensors-0.4.5\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2024.9.11\n",
      "    Uninstalling regex-2024.9.11:\n",
      "      Successfully uninstalled regex-2024.9.11\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.2\n",
      "    Uninstalling PyYAML-6.0.2:\n",
      "      Successfully uninstalled PyYAML-6.0.2\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: pillow 11.0.0\n",
      "    Uninstalling pillow-11.0.0:\n",
      "      Successfully uninstalled pillow-11.0.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.4.2\n",
      "    Uninstalling networkx-3.4.2:\n",
      "      Successfully uninstalled networkx-3.4.2\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.5\n",
      "    Uninstalling MarkupSafe-2.1.5:\n",
      "      Successfully uninstalled MarkupSafe-2.1.5\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.4.2\n",
      "    Uninstalling joblib-1.4.2:\n",
      "      Successfully uninstalled joblib-1.4.2\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.16.1\n",
      "    Uninstalling filelock-3.16.1:\n",
      "      Successfully uninstalled filelock-3.16.1\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.6\n",
      "    Uninstalling colorama-0.4.6:\n",
      "      Successfully uninstalled colorama-0.4.6\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.4.0\n",
      "    Uninstalling charset-normalizer-3.4.0:\n",
      "      Successfully uninstalled charset-normalizer-3.4.0\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.8.30\n",
      "    Uninstalling certifi-2024.8.30:\n",
      "      Successfully uninstalled certifi-2024.8.30\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.5\n",
      "    Uninstalling tqdm-4.66.5:\n",
      "      Successfully uninstalled tqdm-4.66.5\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.4\n",
      "    Uninstalling Jinja2-3.1.4:\n",
      "      Successfully uninstalled Jinja2-3.1.4\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.0\n",
      "    Uninstalling torch-2.5.0:\n",
      "      Successfully uninstalled torch-2.5.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.5.2\n",
      "    Uninstalling scikit-learn-1.5.2:\n",
      "      Successfully uninstalled scikit-learn-1.5.2\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.26.1\n",
      "    Uninstalling huggingface-hub-0.26.1:\n",
      "      Successfully uninstalled huggingface-hub-0.26.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.1\n",
      "    Uninstalling tokenizers-0.20.1:\n",
      "      Successfully uninstalled tokenizers-0.20.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.45.2\n",
      "    Uninstalling transformers-4.45.2:\n",
      "      Successfully uninstalled transformers-4.45.2\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 3.2.1\n",
      "    Uninstalling sentence-transformers-3.2.1:\n",
      "      Successfully uninstalled sentence-transformers-3.2.1\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.0.0 certifi-2024.8.30 charset-normalizer-3.4.0 colorama-0.4.6 filelock-3.16.1 fsspec-2024.10.0 huggingface-hub-0.26.2 idna-3.10 jinja2-3.1.4 joblib-1.4.2 mpmath-1.3.0 networkx-3.4.2 numpy-2.1.3 packaging-24.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 sentence-transformers-3.2.1 sympy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.20.3 torch-2.5.1 tqdm-4.67.0 transformers-4.46.2 typing-extensions-4.12.2 urllib3-2.2.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\mktmi\\anaconda3\\envs\\Projetofinal\\Lib\\site-packages\\~egex'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\mktmi\\anaconda3\\envs\\Projetofinal\\Lib\\site-packages\\~aml'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\mktmi\\anaconda3\\envs\\Projetofinal\\Lib\\site-packages\\~-mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\mktmi\\anaconda3\\envs\\Projetofinal\\Lib\\site-packages\\~-mpy'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\mktmi\\anaconda3\\envs\\Projetofinal\\Lib\\site-packages\\~harset_normalizer'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.0.2 requires fsspec[http]<=2024.9.0,>=2023.1.0, but you have fsspec 2024.10.0 which is incompatible.\n",
      "gradio 5.5.0 requires markupsafe~=2.0, but you have markupsafe 3.0.2 which is incompatible.\n",
      "langchain 0.3.7 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.1.3 which is incompatible.\n",
      "langchain-community 0.3.3 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.1.3 which is incompatible.\n",
      "langchain-openai 0.2.5 requires openai<2.0.0,>=1.52.0, but you have openai 1.11.0 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 2.1.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228fbb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DocArrayInMemorySearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84e16e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting docarray\n",
      "  Using cached docarray-0.40.0-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from docarray) (1.26.4)\n",
      "Requirement already satisfied: orjson>=3.8.2 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from docarray) (3.10.7)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from docarray) (2.9.2)\n",
      "Requirement already satisfied: rich>=13.1.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from docarray) (13.8.1)\n",
      "Collecting types-requests>=2.28.11.6 (from docarray)\n",
      "  Using cached types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from docarray) (0.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic>=1.10.8->docarray) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic>=1.10.8->docarray) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic>=1.10.8->docarray) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rich>=13.1.0->docarray) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rich>=13.1.0->docarray) (2.18.0)\n",
      "Requirement already satisfied: urllib3>=2 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from types-requests>=2.28.11.6->docarray) (2.2.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from typing-inspect>=0.8.0->docarray) (1.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray) (0.1.2)\n",
      "Using cached docarray-0.40.0-py3-none-any.whl (270 kB)\n",
      "Using cached types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: types-requests, docarray\n",
      "Successfully installed docarray-0.40.0 types-requests-2.32.0.20241016\n"
     ]
    }
   ],
   "source": [
    "pip install docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c58bb677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6e1b0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.3.4)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.2.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain) (3.10.8)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain) (0.3.12)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain) (0.1.139)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (4.46.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (2.4.1+cu118)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (0.24.7)\n",
      "Requirement already satisfied: Pillow in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langchain sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cefafa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\O'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\O'\n",
      "C:\\Users\\mktmi\\AppData\\Local\\Temp\\ipykernel_18812\\4135097167.py:2: SyntaxWarning: invalid escape sequence '\\O'\n",
      "  loader = CSVLoader(\"data\\OutdoorClothingCatalog_1000.csv\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "loader = CSVLoader(\"data\\OutdoorClothingCatalog_1000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b31c218f",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mktmi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.document_loaders import CSVLoader\n",
    "loader = CSVLoader(\"data\\\\OutdoorClothingCatalog_1000.csv\", encoding=\"utf-8\")\n",
    "\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={'device': 'cpu'})\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2006054",
   "metadata": {
    "height": 183,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mktmi\\AppData\\Local\\Temp\\ipykernel_18812\\570073578.py:18: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(temperature=0.0)\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load CSV file\n",
    "loader = CSVLoader(\"data\\\\OutdoorClothingCatalog_1000.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Create index\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={'device': 'cpu'})\n",
    ").from_loaders([loader])\n",
    "\n",
    "# Set up ChatOpenAI and RetrievalQA\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=index.vectorstore.as_retriever(),\n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\n",
    "        \"document_separator\": \"<<<<>>>>>\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ebd73",
   "metadata": {},
   "source": [
    "#### Coming up with test datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58bfa971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mktmi\\AppData\\Local\\Temp\\ipykernel_18812\\1764097479.py:34: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response_10 = qa({\"query\": data_10.page_content})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response for data[10]: {'query': \": 10\\nname: Cozy Comfort Pullover Set, Stripe\\ndescription: Perfect for lounging, this striped knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out.\\r\\n\\r\\nSize & Fit\\r\\n- Pants are Favorite Fit: Sits lower on the waist.\\r\\n- Relaxed Fit: Our most generous fit sits farthest from the body.\\r\\n\\r\\nFabric & Care\\r\\n- In the softest blend of 63% polyester, 35% rayon and 2% spandex.\\r\\n\\r\\nAdditional Features\\r\\n- Relaxed fit top with raglan sleeves and rounded hem.\\r\\n- Pull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg.\\r\\n\\r\\nImported.\", 'result': 'The Cozy Comfort Pullover Set, Stripe is made of a soft blend of 63% polyester, 35% rayon, and 2% spandex. The pants have a wide elastic waistband, drawstring, side pockets, and a modern slim leg. The top has a relaxed fit with raglan sleeves and a rounded hem.'}\n",
      "Response for data[11]: {'query': ': 11\\nname: Ultra-Lofty 850 Stretch Down Hooded Jacket\\ndescription: This technical stretch down jacket from our DownTek collection is sure to keep you warm and comfortable with its full-stretch construction providing exceptional range of motion. With a slightly fitted style that falls at the hip and best with a midweight layer, this jacket is suitable for light activity up to 20° and moderate activity up to -30°. The soft and durable 100% polyester shell offers complete windproof protection and is insulated with warm, lofty goose down. Other features include welded baffles for a no-stitch construction and excellent stretch, an adjustable hood, an interior media port and mesh stash pocket and a hem drawcord. Machine wash and dry. Imported.', 'result': 'Based on the description provided, the Ultra-Lofty 850 Stretch Down Hooded Jacket is suitable for light activity up to 20° and moderate activity up to -30°. It features a full-stretch construction for exceptional range of motion, a slightly fitted style falling at the hip, a 100% polyester shell for windproof protection, and warm goose down insulation. Additional features include welded baffles, an adjustable hood, an interior media port, mesh stash pocket, and a hem drawcord. It is machine washable and imported.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load the CSV file\n",
    "loader = CSVLoader(\"data\\\\OutdoorClothingCatalog_1000.csv\", encoding=\"utf-8\")\n",
    "documents = loader.load()  # Load all documents\n",
    "\n",
    "# Access specific data points (e.g., 10th and 11th entries)\n",
    "data_10 = documents[10]\n",
    "data_11 = documents[11]\n",
    "\n",
    "# Set up the index and LLM pipeline\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={'device': 'cpu'})\n",
    ").from_loaders([loader])\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=index.vectorstore.as_retriever(),\n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\n",
    "        \"document_separator\": \"<<<<>>>>>\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Test retrieval with data points\n",
    "response_10 = qa({\"query\": data_10.page_content})\n",
    "response_11 = qa({\"query\": data_11.page_content})\n",
    "\n",
    "print(\"Response for data[10]:\", response_10)\n",
    "print(\"Response for data[11]:\", response_11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d548aef",
   "metadata": {},
   "source": [
    "#### Hard-coded examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "106fbd91-f7bc-4d6b-b090-54b6a485ce39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c5d5af6-36db-4421-b635-46384e677847",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mktmi\\AppData\\Local\\Temp\\ipykernel_18812\\887252583.py:53: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(\n",
      "C:\\Users\\mktmi\\AppData\\Local\\Temp\\ipykernel_18812\\887252583.py:61: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n",
      "C:\\Users\\mktmi\\AppData\\Local\\Temp\\ipykernel_18812\\887252583.py:68: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = llm_chain.run(custom_query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Prompt Result: answer='The Shetland Wool Quarter-Zip Pullover is made from 100% premium Shetland wool and should be handwashed and dried flat.'\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Load the CSV file\n",
    "loader = CSVLoader(\"data\\\\OutdoorClothingCatalog_1000.csv\", encoding=\"utf-8\")\n",
    "documents = loader.load()  # Load all documents\n",
    "\n",
    "# Set up the index and retrieval pipeline\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={'device': 'cpu'})\n",
    ").from_loaders([loader])\n",
    "\n",
    "retriever = index.vectorstore.as_retriever()\n",
    "\n",
    "# Define hard-coded examples and prompt template\n",
    "examples = [\n",
    "    {\"query\": \"Do the Cozy Comfort Pullover Set have side pockets?\", \"answer\": \"Yes\"},\n",
    "    {\"query\": \"What collection is the Ultra-Lofty 850 Stretch Down Hooded Jacket from?\", \"answer\": \"The DownTek collection\"}\n",
    "]\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"Examples:\\n\"\n",
    "             \"1. Query: Do the Cozy Comfort Pullover Set have side pockets?\\n\"\n",
    "             \"   Answer: Yes\\n\"\n",
    "             \"2. Query: What collection is the Ultra-Lofty 850 Stretch Down Hooded Jacket from?\\n\"\n",
    "             \"   Answer: The DownTek collection\\n\"\n",
    "             \"Query: {query}\\n\"\n",
    "             \"Answer:\"\n",
    ")\n",
    "\n",
    "# Define the output model and parser\n",
    "class Answer(BaseModel):\n",
    "    answer: str = Field(description=\"The answer to the query\")\n",
    "\n",
    "class AnswerOutputParser(BaseOutputParser):\n",
    "    def parse(self, text: str) -> Answer:\n",
    "        answer = text.strip().split(\"Answer:\")[-1].strip()\n",
    "        return Answer(answer=answer)\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "\n",
    "# Create the LLMChain with custom prompt and output parser\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    output_parser=AnswerOutputParser()\n",
    ")\n",
    "\n",
    "# Example query and document retrieval\n",
    "query = \"Is the Cozy Comfort Pullover Set available in different colors?\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Combine retrieved documents' content into a single query\n",
    "combined_content = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "custom_query = {\"query\": combined_content}\n",
    "\n",
    "# Run the custom prompt chain\n",
    "result = llm_chain.run(custom_query)\n",
    "\n",
    "# Print the result\n",
    "print(\"Custom Prompt Result:\", result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ce3e4f",
   "metadata": {},
   "source": [
    "#### LLM-Generated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d44f8376",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAGenerateChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34e87816",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acb34772-368f-4b5e-b4bd-da9b637cc7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62abae09",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mktmi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain\\chains\\llm.py:369: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Examples: [{'qa_pairs': {'query': \"What is the approximate weight of the Women's Campside Oxfords per pair?\", 'answer': \"The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz.\"}}, {'qa_pairs': {'query': 'What are the dimensions of the small and medium sizes of the Recycled Waterhog Dog Mat, Chevron Weave?', 'answer': 'The small size has dimensions of 18\" x 28\" and the medium size has dimensions of 22.5\" x 34.5\".'}}, {'qa_pairs': {'query': \"What features does the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece have that make it ideal for young children?\", 'answer': 'The swimsuit features bright colors, ruffles, exclusive whimsical prints, four-way-stretch and chlorine-resistant fabric, UPF 50+ rated fabric for sun protection, crossover no-slip straps, fully lined bottom for secure fit and maximum coverage.'}}, {'qa_pairs': {'query': 'What is the fabric composition of the Refresh Swimwear V-Neck Tankini Contrasts top?', 'answer': 'The body of the swimtop is made of 82% recycled nylon and 18% Lycra® spandex, while the lining is made of 90% recycled nylon and 10% Lycra® spandex.'}}, {'qa_pairs': {'query': 'What technology is used in the EcoFlex 3L Storm Pants to make them more breathable and waterproof?', 'answer': 'The EcoFlex 3L Storm Pants use TEK O2 technology to provide enhanced breathability and waterproof protection.'}}]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.evaluation.qa import QAGenerateChain\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Load the CSV file\n",
    "loader = CSVLoader(\"data\\\\OutdoorClothingCatalog_1000.csv\", encoding=\"utf-8\")\n",
    "documents = loader.load()  # Load all documents\n",
    "\n",
    "# Set up LLM for generating examples\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "\n",
    "# Define a prompt template (if not defined earlier)\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"Examples:\\n\"\n",
    "             \"1. Query: Do the Cozy Comfort Pullover Set have side pockets?\\n\"\n",
    "             \"   Answer: Yes\\n\"\n",
    "             \"2. Query: What collection is the Ultra-Lofty 850 Stretch Down Hooded Jacket from?\\n\"\n",
    "             \"   Answer: The DownTek collection\\n\"\n",
    "             \"Query: {query}\\n\"\n",
    "             \"Answer:\"\n",
    ")\n",
    "\n",
    "# Initialize LLMChain and QAGenerateChain\n",
    "example_gen_chain = QAGenerateChain.from_llm(llm)\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Generate new examples from the first few documents\n",
    "new_examples = example_gen_chain.apply_and_parse(\n",
    "    [{\"doc\": doc.page_content} for doc in documents[:5]]\n",
    ")\n",
    "\n",
    "# Print the generated examples\n",
    "print(\"Generated Examples:\", new_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97ab28b5",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Generated Example: {'qa_pairs': {'query': \"What is the approximate weight of the Women's Campside Oxfords per pair?\", 'answer': \"The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz.\"}}\n"
     ]
    }
   ],
   "source": [
    "print(\"First Generated Example:\", new_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ebe4228",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Document: : 0\n",
      "name: Women's Campside Oxfords\n",
      "description: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \n",
      "\n",
      "Size & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \n",
      "\n",
      "Specs: Approx. weight: 1 lb.1 oz. per pair. \n",
      "\n",
      "Construction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT® antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \n",
      "\n",
      "Questions? Please contact us for any inquiries.\n"
     ]
    }
   ],
   "source": [
    "print(\"First Document:\", documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7693fe86-feeb-4d73-b400-e66e79315274",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': \"What is the approximate weight of the Women's Campside Oxfords per pair?\",\n",
       "  'answer': \"The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz.\"},\n",
       " {'query': 'What are the dimensions of the small and medium sizes of the Recycled Waterhog Dog Mat, Chevron Weave?',\n",
       "  'answer': 'The small size has dimensions of 18\" x 28\" and the medium size has dimensions of 22.5\" x 34.5\".'},\n",
       " {'query': \"What features does the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece have that make it ideal for young children?\",\n",
       "  'answer': 'The swimsuit features bright colors, ruffles, exclusive whimsical prints, four-way-stretch and chlorine-resistant fabric, UPF 50+ rated fabric for sun protection, crossover no-slip straps, fully lined bottom for secure fit and maximum coverage.'},\n",
       " {'query': 'What is the fabric composition of the Refresh Swimwear V-Neck Tankini Contrasts top?',\n",
       "  'answer': 'The body of the swimtop is made of 82% recycled nylon and 18% Lycra® spandex, while the lining is made of 90% recycled nylon and 10% Lycra® spandex.'},\n",
       " {'query': 'What technology is used in the EcoFlex 3L Storm Pants to make them more breathable and waterproof?',\n",
       "  'answer': 'The EcoFlex 3L Storm Pants use TEK O2 technology to provide enhanced breathability and waterproof protection.'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_flattened = [data['qa_pairs'] for data in new_examples]\n",
    "d_flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf25f2f",
   "metadata": {},
   "source": [
    "#### Combine examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ada2a3fc",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# examples += new_example\n",
    "examples += d_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2184b9d7-22ab-43a5-9ba5-b27fef024874",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Do the Cozy Comfort Pullover Set have side pockets?',\n",
       " 'answer': 'Yes'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cdf5cf5",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Do the Cozy Comfort Pullover Set have side pockets?',\n",
       " 'result': 'Yes, the Cozy Comfort Pullover Set has side seam pockets and a back zip pocket, as well as two elastic mesh water bottle pockets.'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.invoke(examples[0][\"query\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f3cb08",
   "metadata": {},
   "source": [
    "### Manual Evaluation - Fun part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcaf622e",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a142638",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RetrievalQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"Do the Cozy Comfort Pullover Set have side pockets?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Do the Cozy Comfort Pullover Set have side pockets?\",\n",
      "  \"context\": \"Side seam pockets and back zip pocket, with mesh insert for quick drainage.<<<<>>>>>Two elastic mesh water bottle pockets.\\r\\nTop compartment includes pocket with double-seal zipper for quick access.\\r\\nSide<<<<>>>>>All pockets have sturdy pocket bags and offer plenty of room for a wallet, cell phone and more.\\r\\n\\r\\nGusseted crotch for ease of movement.\\r\\n\\r\\nImported.<<<<>>>>>Two elastic mesh water bottle pockets.\\r\\nTop compartment includes pocket with double-se\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nSide seam pockets and back zip pocket, with mesh insert for quick drainage.<<<<>>>>>Two elastic mesh water bottle pockets.\\r\\nTop compartment includes pocket with double-seal zipper for quick access.\\r\\nSide<<<<>>>>>All pockets have sturdy pocket bags and offer plenty of room for a wallet, cell phone and more.\\r\\n\\r\\nGusseted crotch for ease of movement.\\r\\n\\r\\nImported.<<<<>>>>>Two elastic mesh water bottle pockets.\\r\\nTop compartment includes pocket with double-se\\nHuman: Do the Cozy Comfort Pullover Set have side pockets?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain > chain:LLMChain > llm:ChatOpenAI] [712ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Yes, the Cozy Comfort Pullover Set has side seam pockets and a back zip pocket, as well as two elastic mesh water bottle pockets.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Yes, the Cozy Comfort Pullover Set has side seam pockets and a back zip pocket, as well as two elastic mesh water bottle pockets.\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"completion_tokens\": 29,\n",
      "                \"total_tokens\": 186,\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"cached_tokens\": 0,\n",
      "                  \"audio_tokens\": 0\n",
      "                },\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-a336f57b-64a9-407b-9e1f-be614862fba4-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 157,\n",
      "      \"completion_tokens\": 29,\n",
      "      \"total_tokens\": 186,\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"cached_tokens\": 0,\n",
      "        \"audio_tokens\": 0\n",
      "      },\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain > chain:LLMChain] [714ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Yes, the Cozy Comfort Pullover Set has side seam pockets and a back zip pocket, as well as two elastic mesh water bottle pockets.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain] [714ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"Yes, the Cozy Comfort Pullover Set has side seam pockets and a back zip pocket, as well as two elastic mesh water bottle pockets.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RetrievalQA] [731ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"result\": \"Yes, the Cozy Comfort Pullover Set has side seam pockets and a back zip pocket, as well as two elastic mesh water bottle pockets.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Do the Cozy Comfort Pullover Set have side pockets?',\n",
       " 'result': 'Yes, the Cozy Comfort Pullover Set has side seam pockets and a back zip pocket, as well as two elastic mesh water bottle pockets.'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.invoke(examples[0][\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b3d6bef0",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Turn off the debug mode\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bdbdce",
   "metadata": {},
   "source": [
    "### LLM assisted evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a54769b0-3daf-4cac-b259-89a10dd9b5a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples += d_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ea95385-1b4c-440a-9fea-8500b4cc2154",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'Do the Cozy Comfort Pullover Set have side pockets?',\n",
       "  'answer': 'Yes'},\n",
       " {'query': 'What collection is the Ultra-Lofty 850 Stretch Down Hooded Jacket from?',\n",
       "  'answer': 'The DownTek collection'},\n",
       " {'query': \"What is the approximate weight of the Women's Campside Oxfords per pair?\",\n",
       "  'answer': \"The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz.\"},\n",
       " {'query': 'What are the dimensions of the small and medium sizes of the Recycled Waterhog Dog Mat, Chevron Weave?',\n",
       "  'answer': 'The small size has dimensions of 18\" x 28\" and the medium size has dimensions of 22.5\" x 34.5\".'},\n",
       " {'query': \"What features does the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece have that make it ideal for young children?\",\n",
       "  'answer': 'The swimsuit features bright colors, ruffles, exclusive whimsical prints, four-way-stretch and chlorine-resistant fabric, UPF 50+ rated fabric for sun protection, crossover no-slip straps, fully lined bottom for secure fit and maximum coverage.'},\n",
       " {'query': 'What is the fabric composition of the Refresh Swimwear V-Neck Tankini Contrasts top?',\n",
       "  'answer': 'The body of the swimtop is made of 82% recycled nylon and 18% Lycra® spandex, while the lining is made of 90% recycled nylon and 10% Lycra® spandex.'},\n",
       " {'query': 'What technology is used in the EcoFlex 3L Storm Pants to make them more breathable and waterproof?',\n",
       "  'answer': 'The EcoFlex 3L Storm Pants use TEK O2 technology to provide enhanced breathability and waterproof protection.'},\n",
       " {'query': \"What is the approximate weight of the Women's Campside Oxfords per pair?\",\n",
       "  'answer': \"The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz.\"},\n",
       " {'query': 'What are the dimensions of the small and medium sizes of the Recycled Waterhog Dog Mat, Chevron Weave?',\n",
       "  'answer': 'The small size has dimensions of 18\" x 28\" and the medium size has dimensions of 22.5\" x 34.5\".'},\n",
       " {'query': \"What features does the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece have that make it ideal for young children?\",\n",
       "  'answer': 'The swimsuit features bright colors, ruffles, exclusive whimsical prints, four-way-stretch and chlorine-resistant fabric, UPF 50+ rated fabric for sun protection, crossover no-slip straps, fully lined bottom for secure fit and maximum coverage.'},\n",
       " {'query': 'What is the fabric composition of the Refresh Swimwear V-Neck Tankini Contrasts top?',\n",
       "  'answer': 'The body of the swimtop is made of 82% recycled nylon and 18% Lycra® spandex, while the lining is made of 90% recycled nylon and 10% Lycra® spandex.'},\n",
       " {'query': 'What technology is used in the EcoFlex 3L Storm Pants to make them more breathable and waterproof?',\n",
       "  'answer': 'The EcoFlex 3L Storm Pants use TEK O2 technology to provide enhanced breathability and waterproof protection.'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a4dca05a",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "predictions = qa.batch(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae7e8b4e-4468-4048-8544-c9936704ea93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'Do the Cozy Comfort Pullover Set have side pockets?',\n",
       "  'answer': 'Yes',\n",
       "  'result': 'Yes, the Cozy Comfort Pullover Set has side seam pockets and a back zip pocket, as well as two elastic mesh water bottle pockets.'},\n",
       " {'query': 'What collection is the Ultra-Lofty 850 Stretch Down Hooded Jacket from?',\n",
       "  'answer': 'The DownTek collection',\n",
       "  'result': 'The Ultra-Lofty 850 Stretch Down Hooded Jacket is from the DownTek collection.'},\n",
       " {'query': \"What is the approximate weight of the Women's Campside Oxfords per pair?\",\n",
       "  'answer': \"The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz.\",\n",
       "  'result': \"The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz.\"},\n",
       " {'query': 'What are the dimensions of the small and medium sizes of the Recycled Waterhog Dog Mat, Chevron Weave?',\n",
       "  'answer': 'The small size has dimensions of 18\" x 28\" and the medium size has dimensions of 22.5\" x 34.5\".',\n",
       "  'result': 'The dimensions of the small size of the Recycled Waterhog Dog Mat, Chevron Weave are 18\" x 28\", and the dimensions of the medium size are 22.5\" x 34.5\".'},\n",
       " {'query': \"What features does the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece have that make it ideal for young children?\",\n",
       "  'answer': 'The swimsuit features bright colors, ruffles, exclusive whimsical prints, four-way-stretch and chlorine-resistant fabric, UPF 50+ rated fabric for sun protection, crossover no-slip straps, fully lined bottom for secure fit and maximum coverage.',\n",
       "  'result': \"The Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece is ideal for young children because it has features like bright colors, ruffles, exclusive whimsical prints, four-way-stretch and chlorine-resistant fabric, UPF 50+ sun protection, crossover no-slip straps, fully lined bottom for secure fit and maximum coverage.\"},\n",
       " {'query': 'What is the fabric composition of the Refresh Swimwear V-Neck Tankini Contrasts top?',\n",
       "  'answer': 'The body of the swimtop is made of 82% recycled nylon and 18% Lycra® spandex, while the lining is made of 90% recycled nylon and 10% Lycra® spandex.',\n",
       "  'result': 'The Refresh Swimwear V-Neck Tankini Contrasts top is made of 82% recycled nylon with 18% Lycra® spandex for the body, and 90% recycled nylon with 10% Lycra® spandex for the lining.'},\n",
       " {'query': 'What technology is used in the EcoFlex 3L Storm Pants to make them more breathable and waterproof?',\n",
       "  'answer': 'The EcoFlex 3L Storm Pants use TEK O2 technology to provide enhanced breathability and waterproof protection.',\n",
       "  'result': 'The EcoFlex 3L Storm Pants use TEK O2 technology to make them more breathable and waterproof.'},\n",
       " {'query': \"What is the approximate weight of the Women's Campside Oxfords per pair?\",\n",
       "  'answer': \"The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz.\",\n",
       "  'result': \"The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz.\"},\n",
       " {'query': 'What are the dimensions of the small and medium sizes of the Recycled Waterhog Dog Mat, Chevron Weave?',\n",
       "  'answer': 'The small size has dimensions of 18\" x 28\" and the medium size has dimensions of 22.5\" x 34.5\".',\n",
       "  'result': 'The dimensions of the small size of the Recycled Waterhog Dog Mat, Chevron Weave are 18\" x 28\", and the dimensions of the medium size are 22.5\" x 34.5\".'},\n",
       " {'query': \"What features does the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece have that make it ideal for young children?\",\n",
       "  'answer': 'The swimsuit features bright colors, ruffles, exclusive whimsical prints, four-way-stretch and chlorine-resistant fabric, UPF 50+ rated fabric for sun protection, crossover no-slip straps, fully lined bottom for secure fit and maximum coverage.',\n",
       "  'result': \"The Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece is ideal for young children because it has features like bright colors, ruffles, exclusive whimsical prints, four-way-stretch and chlorine-resistant fabric, UPF 50+ sun protection, crossover no-slip straps, fully lined bottom for secure fit and maximum coverage.\"},\n",
       " {'query': 'What is the fabric composition of the Refresh Swimwear V-Neck Tankini Contrasts top?',\n",
       "  'answer': 'The body of the swimtop is made of 82% recycled nylon and 18% Lycra® spandex, while the lining is made of 90% recycled nylon and 10% Lycra® spandex.',\n",
       "  'result': 'The Refresh Swimwear V-Neck Tankini Contrasts top is made of 82% recycled nylon with 18% Lycra® spandex for the body, and 90% recycled nylon with 10% Lycra® spandex for the lining.'},\n",
       " {'query': 'What technology is used in the EcoFlex 3L Storm Pants to make them more breathable and waterproof?',\n",
       "  'answer': 'The EcoFlex 3L Storm Pants use TEK O2 technology to provide enhanced breathability and waterproof protection.',\n",
       "  'result': 'The EcoFlex 3L Storm Pants use TEK O2 technology to make them more breathable and waterproof.'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6012a3e0",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "724b1c0b",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "eval_chain = QAEvalChain.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8b46ae55",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "graded_outputs = eval_chain.evaluate(examples, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc42eb35-c2d7-4581-8004-d315ade63eef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'results': 'CORRECT'},\n",
       " {'results': 'CORRECT'},\n",
       " {'results': 'CORRECT'},\n",
       " {'results': 'CORRECT'},\n",
       " {'results': 'CORRECT'},\n",
       " {'results': 'CORRECT'},\n",
       " {'results': 'CORRECT'},\n",
       " {'results': 'CORRECT'},\n",
       " {'results': 'CORRECT'},\n",
       " {'results': 'CORRECT'},\n",
       " {'results': 'CORRECT'},\n",
       " {'results': 'CORRECT'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3437cfbe",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "Question: Do the Cozy Comfort Pullover Set have side pockets?\n",
      "Real Answer: Yes\n",
      "Predicted Answer: Yes, the Cozy Comfort Pullover Set has side seam pockets and a back zip pocket, as well as two elastic mesh water bottle pockets.\n",
      "\n",
      "Example 1:\n",
      "Question: What collection is the Ultra-Lofty 850 Stretch Down Hooded Jacket from?\n",
      "Real Answer: The DownTek collection\n",
      "Predicted Answer: The Ultra-Lofty 850 Stretch Down Hooded Jacket is from the DownTek collection.\n",
      "\n",
      "Example 2:\n",
      "Question: What is the approximate weight of the Women's Campside Oxfords per pair?\n",
      "Real Answer: The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz.\n",
      "Predicted Answer: The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz.\n",
      "\n",
      "Example 3:\n",
      "Question: What are the dimensions of the small and medium sizes of the Recycled Waterhog Dog Mat, Chevron Weave?\n",
      "Real Answer: The small size has dimensions of 18\" x 28\" and the medium size has dimensions of 22.5\" x 34.5\".\n",
      "Predicted Answer: The dimensions of the small size of the Recycled Waterhog Dog Mat, Chevron Weave are 18\" x 28\", and the dimensions of the medium size are 22.5\" x 34.5\".\n",
      "\n",
      "Example 4:\n",
      "Question: What features does the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece have that make it ideal for young children?\n",
      "Real Answer: The swimsuit features bright colors, ruffles, exclusive whimsical prints, four-way-stretch and chlorine-resistant fabric, UPF 50+ rated fabric for sun protection, crossover no-slip straps, fully lined bottom for secure fit and maximum coverage.\n",
      "Predicted Answer: The Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece is ideal for young children because it has features like bright colors, ruffles, exclusive whimsical prints, four-way-stretch and chlorine-resistant fabric, UPF 50+ sun protection, crossover no-slip straps, fully lined bottom for secure fit and maximum coverage.\n",
      "\n",
      "Example 5:\n",
      "Question: What is the fabric composition of the Refresh Swimwear V-Neck Tankini Contrasts top?\n",
      "Real Answer: The body of the swimtop is made of 82% recycled nylon and 18% Lycra® spandex, while the lining is made of 90% recycled nylon and 10% Lycra® spandex.\n",
      "Predicted Answer: The Refresh Swimwear V-Neck Tankini Contrasts top is made of 82% recycled nylon with 18% Lycra® spandex for the body, and 90% recycled nylon with 10% Lycra® spandex for the lining.\n",
      "\n",
      "Example 6:\n",
      "Question: What technology is used in the EcoFlex 3L Storm Pants to make them more breathable and waterproof?\n",
      "Real Answer: The EcoFlex 3L Storm Pants use TEK O2 technology to provide enhanced breathability and waterproof protection.\n",
      "Predicted Answer: The EcoFlex 3L Storm Pants use TEK O2 technology to make them more breathable and waterproof.\n",
      "\n",
      "Example 7:\n",
      "Question: What is the approximate weight of the Women's Campside Oxfords per pair?\n",
      "Real Answer: The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz.\n",
      "Predicted Answer: The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz.\n",
      "\n",
      "Example 8:\n",
      "Question: What are the dimensions of the small and medium sizes of the Recycled Waterhog Dog Mat, Chevron Weave?\n",
      "Real Answer: The small size has dimensions of 18\" x 28\" and the medium size has dimensions of 22.5\" x 34.5\".\n",
      "Predicted Answer: The dimensions of the small size of the Recycled Waterhog Dog Mat, Chevron Weave are 18\" x 28\", and the dimensions of the medium size are 22.5\" x 34.5\".\n",
      "\n",
      "Example 9:\n",
      "Question: What features does the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece have that make it ideal for young children?\n",
      "Real Answer: The swimsuit features bright colors, ruffles, exclusive whimsical prints, four-way-stretch and chlorine-resistant fabric, UPF 50+ rated fabric for sun protection, crossover no-slip straps, fully lined bottom for secure fit and maximum coverage.\n",
      "Predicted Answer: The Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece is ideal for young children because it has features like bright colors, ruffles, exclusive whimsical prints, four-way-stretch and chlorine-resistant fabric, UPF 50+ sun protection, crossover no-slip straps, fully lined bottom for secure fit and maximum coverage.\n",
      "\n",
      "Example 10:\n",
      "Question: What is the fabric composition of the Refresh Swimwear V-Neck Tankini Contrasts top?\n",
      "Real Answer: The body of the swimtop is made of 82% recycled nylon and 18% Lycra® spandex, while the lining is made of 90% recycled nylon and 10% Lycra® spandex.\n",
      "Predicted Answer: The Refresh Swimwear V-Neck Tankini Contrasts top is made of 82% recycled nylon with 18% Lycra® spandex for the body, and 90% recycled nylon with 10% Lycra® spandex for the lining.\n",
      "\n",
      "Example 11:\n",
      "Question: What technology is used in the EcoFlex 3L Storm Pants to make them more breathable and waterproof?\n",
      "Real Answer: The EcoFlex 3L Storm Pants use TEK O2 technology to provide enhanced breathability and waterproof protection.\n",
      "Predicted Answer: The EcoFlex 3L Storm Pants use TEK O2 technology to make them more breathable and waterproof.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, eg in enumerate(examples):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"Question: \" + predictions[i]['query'])\n",
    "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
    "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
    "    # print(\"Predicted Grade: \" + graded_outputs[i]['text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d127a-a9e3-465d-a8ae-0e2c4b4a2659",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "One can also easily evaluate your QA chains with the metrics offered in ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a5ef0493-34ff-4801-b405-69c76ce86c38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load the text data\n",
    "loader = TextLoader(\"data/nyc_text.txt\", encoding=\"utf-8\")\n",
    "\n",
    "# Set up the index with HuggingFace embeddings on CPU\n",
    "index = VectorstoreIndexCreator(\n",
    "    embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={'device': 'cpu'})\n",
    ").from_loaders([loader])\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Set up the QA retrieval chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=index.vectorstore.as_retriever(),\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f0449cae-de25-4ef6-ae64-78ccf5e06a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New York City was originally named New Amsterdam by Dutch colonists in 1626. When the city came under British control in 1664, it was renamed New York after King Charles II of England granted the lands to his brother, the Duke of York. The city has been continuously named New York since November 1674.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing it out\n",
    "\n",
    "question = \"How did New York City get its name?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e846b3d-f79f-46eb-8075-c816268c0500",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How did New York City get its name?',\n",
       " 'result': 'New York City was originally named New Amsterdam by Dutch colonists in 1626. When the city came under British control in 1664, it was renamed New York after King Charles II of England granted the lands to his brother, the Duke of York. The city has been continuously named New York since November 1674.',\n",
       " 'source_documents': [Document(id='b4a63449-a47a-489d-aa10-e5db0090369c', metadata={'source': 'data/nyc_text.txt'}, page_content='The city and its metropolitan area constitute the premier gateway for legal immigration to the United States. As many as 800 languages are spoken in New York, making it the most linguistically diverse city in the world. New York City is home to more than 3.2 million residents born outside the U.S., the largest foreign-born population of any city in the world as of 2016.New York City traces its origins to a trading post founded on the southern tip of Manhattan Island by Dutch colonists in approximately 1624. The settlement was named New Amsterdam (Dutch: Nieuw Amsterdam) in 1626 and was chartered as a city in 1653. The city came under British control in 1664 and was renamed New York after King Charles II of England granted the lands to his brother, the Duke of York. The city was regained by the Dutch in July 1673 and was renamed New Orange for one year and three months; the city has been continuously named New York since November 1674. New York City was the capital of the United States'),\n",
       "  Document(id='fe496746-b8ae-4da3-943e-2a09c18887af', metadata={'source': 'data/nyc_text.txt'}, page_content='New York City has been a metropolitan municipality with a Strong mayor–council form of government since its consolidation in 1898. In New York City, the city government is responsible for public education, correctional institutions, public safety, recreational facilities, sanitation, water supply, and welfare services.'),\n",
       "  Document(id='6014fdb2-0a24-4097-84a6-40419d5f261e', metadata={'source': 'data/nyc_text.txt'}, page_content=\"Despite New York's heavy reliance on its vast public transit system, streets are a defining feature of the city. The Commissioners' Plan of 1811 greatly influenced the city's physical development. Several of the city's streets and avenues, including Broadway, Wall Street, Madison Avenue, and Seventh Avenue are also used as metonyms for national industries there: the theater, finance, advertising, and fashion organizations, respectively.\"),\n",
       "  Document(id='f9cb122d-51c3-480e-8953-cb867a307525', metadata={'source': 'data/nyc_text.txt'}, page_content=\"In the pre-Columbian era, the area of present-day New York City was inhabited by Algonquian Native Americans, including the Lenape. Their homeland, known as Lenapehoking, included the present-day areas of Staten Island, Manhattan, the Bronx, the western portion of Long Island (including the areas that would later become the boroughs of Brooklyn and Queens), and the Lower Hudson Valley.The first documented visit into New York Harbor by a European was in 1524 by Italian Giovanni da Verrazzano, an explorer from Florence in the service of the French crown. He claimed the area for France and named it Nouvelle Angoulême (New Angoulême). A Spanish expedition, led by the Portuguese captain Estêvão Gomes sailing for Emperor Charles V, arrived in New York Harbor in January 1525 and charted the mouth of the Hudson River, which he named Río de San Antonio ('Saint Anthony's River'). The Padrón Real of 1527, the first scientific map to show the East Coast of North America continuously, was informed\")]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d9da8-a593-4fc6-9d4b-fea2af6bdfd0",
   "metadata": {},
   "source": [
    "Now in order to evaluate the qa system we generated a few relevant questions. We've generated a few question for you but feel free to add any you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a2e2cade-0005-41c1-b775-c6a7175bcf3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_questions = [\n",
    "    \"What is the population of New York City as of 2020?\",\n",
    "    \"Which borough of New York City has the highest population?\",\n",
    "    \"What is the economic significance of New York City?\",\n",
    "    \"How did New York City get its name?\",\n",
    "    \"What is the significance of the Statue of Liberty in New York City?\",\n",
    "]\n",
    "\n",
    "eval_answers = [\n",
    "    \"8,804,190\",\n",
    "    \"Brooklyn\",\n",
    "    \"New York City's economic significance is vast, as it serves as the global financial capital, housing Wall Street and major financial institutions. Its diverse economy spans technology, media, healthcare, education, and more, making it resilient to economic fluctuations. NYC is a hub for international business, attracting global companies, and boasts a large, skilled labor force. Its real estate market, tourism, cultural industries, and educational institutions further fuel its economic prowess. The city's transportation network and global influence amplify its impact on the world stage, solidifying its status as a vital economic player and cultural epicenter.\",\n",
    "    \"New York City got its name when it came under British control in 1664. King Charles II of England granted the lands to his brother, the Duke of York, who named the city New York in his own honor.\",\n",
    "    \"The Statue of Liberty in New York City holds great significance as a symbol of the United States and its ideals of liberty and peace. It greeted millions of immigrants who arrived in the U.S. by ship in the late 19th and early 20th centuries, representing hope and freedom for those seeking a better life. It has since become an iconic landmark and a global symbol of cultural diversity and freedom.\",\n",
    "]\n",
    "\n",
    "examples = [\n",
    "    {\"query\": q, \"ground_truths\": [eval_answers[i]]}\n",
    "    for i, q in enumerate(eval_questions)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac9358e-f8bc-4992-aea3-c83160ff0ab0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'What is the population of New York City as of 2020?',\n",
       "  'ground_truths': ['8,804,190']},\n",
       " {'query': 'Which borough of New York City has the highest population?',\n",
       "  'ground_truths': ['Brooklyn']},\n",
       " {'query': 'What is the economic significance of New York City?',\n",
       "  'ground_truths': [\"New York City's economic significance is vast, as it serves as the global financial capital, housing Wall Street and major financial institutions. Its diverse economy spans technology, media, healthcare, education, and more, making it resilient to economic fluctuations. NYC is a hub for international business, attracting global companies, and boasts a large, skilled labor force. Its real estate market, tourism, cultural industries, and educational institutions further fuel its economic prowess. The city's transportation network and global influence amplify its impact on the world stage, solidifying its status as a vital economic player and cultural epicenter.\"]},\n",
       " {'query': 'How did New York City get its name?',\n",
       "  'ground_truths': ['New York City got its name when it came under British control in 1664. King Charles II of England granted the lands to his brother, the Duke of York, who named the city New York in his own honor.']},\n",
       " {'query': 'What is the significance of the Statue of Liberty in New York City?',\n",
       "  'ground_truths': ['The Statue of Liberty in New York City holds great significance as a symbol of the United States and its ideals of liberty and peace. It greeted millions of immigrants who arrived in the U.S. by ship in the late 19th and early 20th centuries, representing hope and freedom for those seeking a better life. It has since become an iconic landmark and a global symbol of cultural diversity and freedom.']}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f79c5968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mktmi\\AppData\\Local\\Temp\\ipykernel_18856\\3067862248.py:12: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={'device': 'cpu'})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mktmi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "C:\\Users\\mktmi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain\\indexes\\vectorstore.py:128: UserWarning: Using InMemoryVectorStore as the default vectorstore.This memory store won't persist data. You should explicitlyspecify a vectorstore when using VectorstoreIndexCreator\n",
      "  warnings.warn(\n",
      "C:\\Users\\mktmi\\AppData\\Local\\Temp\\ipykernel_18856\\3067862248.py:16: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(temperature=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York City was originally named New Amsterdam by Dutch colonists in 1626. When the city came under British control in 1664, it was renamed New York after King Charles II of England granted the lands to his brother, the Duke of York. The city has been continuously named New York since November 1674.\n",
      "[{'query': 'What is the population of New York City as of 2020?', 'ground_truths': ['8,804,190']}, {'query': 'Which borough of New York City has the highest population?', 'ground_truths': ['Brooklyn']}, {'query': 'What is the economic significance of New York City?', 'ground_truths': [\"New York City's economic significance is vast, as it serves as the global financial capital, housing Wall Street and major financial institutions. Its diverse economy spans technology, media, healthcare, education, and more, making it resilient to economic fluctuations. NYC is a hub for international business, attracting global companies, and boasts a large, skilled labor force. Its real estate market, tourism, cultural industries, and educational institutions further fuel its economic prowess. The city's transportation network and global influence amplify its impact on the world stage, solidifying its status as a vital economic player and cultural epicenter.\"]}, {'query': 'How did New York City get its name?', 'ground_truths': ['New York City got its name when it came under British control in 1664. King Charles II of England granted the lands to his brother, the Duke of York, who named the city New York in his own honor.']}, {'query': 'What is the significance of the Statue of Liberty in New York City?', 'ground_truths': ['The Statue of Liberty in New York City holds great significance as a symbol of the United States and its ideals of liberty and peace. It greeted millions of immigrants who arrived in the U.S. by ship in the late 19th and early 20th centuries, representing hope and freedom for those seeking a better life. It has since become an iconic landmark and a global symbol of cultural diversity and freedom.']}]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load the text data\n",
    "loader = TextLoader(\"data/nyc_text.txt\", encoding=\"utf-8\")\n",
    "\n",
    "# Set up the index with HuggingFace embeddings on CPU\n",
    "index = VectorstoreIndexCreator(\n",
    "    embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={'device': 'cpu'})\n",
    ").from_loaders([loader])\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Set up the QA retrieval chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=index.vectorstore.as_retriever(),\n",
    "    return_source_documents=True,\n",
    ") \n",
    "\n",
    "# Testing it out\n",
    "question = \"How did New York City get its name?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(result[\"result\"])\n",
    "\n",
    "# Evaluate the QA system\n",
    "eval_questions = [\n",
    "    \"What is the population of New York City as of 2020?\",\n",
    "    \"Which borough of New York City has the highest population?\",\n",
    "    \"What is the economic significance of New York City?\",\n",
    "    \"How did New York City get its name?\",\n",
    "    \"What is the significance of the Statue of Liberty in New York City?\",\n",
    "]\n",
    "\n",
    "eval_answers = [\n",
    "    \"8,804,190\",\n",
    "    \"Brooklyn\",\n",
    "    \"New York City's economic significance is vast, as it serves as the global financial capital, housing Wall Street and major financial institutions. Its diverse economy spans technology, media, healthcare, education, and more, making it resilient to economic fluctuations. NYC is a hub for international business, attracting global companies, and boasts a large, skilled labor force. Its real estate market, tourism, cultural industries, and educational institutions further fuel its economic prowess. The city's transportation network and global influence amplify its impact on the world stage, solidifying its status as a vital economic player and cultural epicenter.\",\n",
    "    \"New York City got its name when it came under British control in 1664. King Charles II of England granted the lands to his brother, the Duke of York, who named the city New York in his own honor.\",\n",
    "    \"The Statue of Liberty in New York City holds great significance as a symbol of the United States and its ideals of liberty and peace. It greeted millions of immigrants who arrived in the U.S. by ship in the late 19th and early 20th centuries, representing hope and freedom for those seeking a better life. It has since become an iconic landmark and a global symbol of cultural diversity and freedom.\",\n",
    "]\n",
    "\n",
    "# Structure examples with questions and corresponding answers\n",
    "examples = [\n",
    "    {\"query\": q, \"ground_truths\": [eval_answers[i]]}\n",
    "    for i, q in enumerate(eval_questions)\n",
    "]\n",
    "\n",
    "print(examples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a21efe8-7c30-449a-9b8e-5b79778e305b",
   "metadata": {},
   "source": [
    "#### Introducing RagasEvaluatorChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c2214-a6eb-4d4f-9403-7e1574b97a36",
   "metadata": {},
   "source": [
    "`RagasEvaluatorChain` creates a wrapper around the metrics ragas provides (documented [here](https://github.com/explodinggradients/ragas/blob/main/docs/metrics.md)), making it easier to run these evaluation with langchain and langsmith.\n",
    "\n",
    "The evaluator chain has the following APIs\n",
    "\n",
    "- `__call__()`: call the `RagasEvaluatorChain` directly on the result of a QA chain.\n",
    "- `evaluate()`: evaluate on a list of examples (with the input queries) and predictions (outputs from the QA chain). \n",
    "- `evaluate_run()`: method implemented that is called by langsmith evaluators to evaluate langsmith datasets.\n",
    "\n",
    "lets see each of them in action to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "022c8aae-fe5f-4274-b638-9209151b9491",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Manhattan (New York County) has the highest population density of any borough in New York City.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = qa_chain.invoke({\"query\": eval_questions[1]})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eae31c80-42c9-4b1f-95b3-c05ceadb103f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_mapping = {\n",
    "    \"query\": \"question\",\n",
    "    \"result\": \"answer\",\n",
    "    \"source_documents\": \"contexts\"\n",
    "}\n",
    "\n",
    "result_updated = {}\n",
    "for old_key, new_key in key_mapping.items():\n",
    "    if old_key in result:\n",
    "        result_updated[new_key] = result[old_key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ecd4dd9f-16d7-43d4-ac8e-6c5aa5e3f7b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Which borough of New York City has the highest population?',\n",
       " 'answer': 'Manhattan (New York County) has the highest population density of any borough in New York City.',\n",
       " 'contexts': [Document(id='b60906f4-fb6d-4a48-8217-582578b1069c', metadata={'source': 'data/nyc_text.txt'}, page_content=\"New York City is the most populous city in the United States, with 8,804,190 residents incorporating more immigration into the city than outmigration since the 2010 United States census. More than twice as many people live in New York City as compared to Los Angeles, the second-most populous U.S. city; and New York has more than three times the population of Chicago, the third-most populous U.S. city. New York City gained more residents between 2010 and 2020 (629,000) than any other U.S. city, and a greater amount than the total sum of the gains over the same decade of the next four largest U.S. cities, Los Angeles, Chicago, Houston, and Phoenix, Arizona combined. New York City's population is about 44% of New York State's population, and about 39% of the population of the New York metropolitan area. The majority of New York City residents in 2020 (5,141,538, or 58.4%) were living on Long Island, in Brooklyn, or in Queens. The New York City metropolitan statistical area, has the\"),\n",
       "  Document(id='dc5a212f-13aa-4a54-abb5-619a0b80d383', metadata={'source': 'data/nyc_text.txt'}, page_content=\"New York, often called New York City or NYC, is the most populous city in the United States. With a 2020 population of 8,804,190 distributed over 300.46 square miles (778.2 km2), New York City is the most densely populated major city in the United States and more than twice as populous as Los Angeles, the nation's second-largest city. New York City is located at the southern tip of New York State. It constitutes the geographical and demographic center of both the Northeast megalopolis and the New York metropolitan area, the largest metropolitan area in the U.S. by both population and urban area. With over 20.1 million people in its metropolitan statistical area and 23.5 million in its combined statistical area as of 2020, New York is one of the world's most populous megacities, and over 58 million people live within 250 mi (400 km) of the city. New York City is a global cultural, financial, entertainment, and media center with a significant influence on commerce, health care and life\"),\n",
       "  Document(id='532f4c94-8b4d-4e34-9c73-f8d332988964', metadata={'source': 'data/nyc_text.txt'}, page_content=\"Manhattan (New York County) is the geographically smallest and most densely populated borough. It is home to Central Park and most of the city's skyscrapers, and is sometimes locally known as The City. Manhattan's population density of 72,033 people per square mile (27,812/km2) in 2015 makes it the highest of any county in the United States and higher than the density of any individual American city.Manhattan is the cultural, administrative, and financial center of New York City and contains the headquarters of many major multinational corporations, the United Nations headquarters, Wall Street, and a number of important universities. The borough of Manhattan is often described as the financial and cultural center of the world.Most of the borough is situated on Manhattan Island, at the mouth of the Hudson River and the East River, and its southern tip, at the confluence of the two rivers, represents the birthplace of New York City itself. Several small islands also compose part of the\"),\n",
       "  Document(id='02468a64-74df-46ff-8895-f60490f7047b', metadata={'source': 'data/nyc_text.txt'}, page_content=\"=== Population density ===\\n\\nIn 2020, the city had an estimated population density of 29,302.37 inhabitants per square mile (11,313.71/km2), rendering it the nation's most densely populated of all larger municipalities (those with more than 100,000 residents), with several small cities (of fewer than 100,000) in adjacent Hudson County, New Jersey having greater density, as per the 2010 census. Geographically co-extensive with New York County, the borough of Manhattan's 2017 population density of 72,918 inhabitants per square mile (28,154/km2) makes it the highest of any county in the United States and higher than the density of any individual American city. The next three densest counties in the United States, placing second through fourth, are also New York boroughs: Brooklyn, the Bronx, and Queens respectively.\\n\\n\\n=== Race and ethnicity ===\")]}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "52d3a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Example usage\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a8b636-d738-41bd-ac68-21cce2c4b720",
   "metadata": {},
   "source": [
    "1. `__call__()`\n",
    "\n",
    "Directly run the evaluation chain with the results from the QA chain. Do note that metrics like context_relevancy and faithfulness require the `source_documents` to be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7815671c-1fc8-46ba-8356-4a0bd5558530",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Recheck the result that we are going to validate.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mresult\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "# Recheck the result that we are going to validate.\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfdd07c-956c-458f-8844-e056f29e3c83",
   "metadata": {},
   "source": [
    "**Faithfulness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "304f5f0f-a237-4584-becb-a35607caf26b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'faithfulness_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m eval_result \u001b[38;5;241m=\u001b[39m \u001b[43mfaithfulness_chain\u001b[49m(result_updated)\n\u001b[0;32m      2\u001b[0m eval_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaithfulness_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'faithfulness_chain' is not defined"
     ]
    }
   ],
   "source": [
    "eval_result = faithfulness_chain(result_updated)\n",
    "eval_result[\"faithfulness_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd78a9-d7c5-42a0-8705-4c544ec6408e",
   "metadata": {},
   "source": [
    "High faithfulness_score means that there are exact consistency between the source documents and the answer.\n",
    "\n",
    "You can check lower faithfulness scores by changing the result (answer from LLM) or source_documents to something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f42b97-a84f-4015-9da5-fa3b0b703c24",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fake_result \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      2\u001b[0m fake_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwe are the champions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m eval_result \u001b[38;5;241m=\u001b[39m faithfulness_chain(fake_result)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "fake_result = result.copy()\n",
    "fake_result[\"result\"] = \"we are the champions\"\n",
    "eval_result = faithfulness_chain(fake_result)\n",
    "eval_result[\"faithfulness_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02348380-159a-4578-a95e-493cdcfcebe7",
   "metadata": {},
   "source": [
    "**Context Relevancy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fd2fb9e-19d1-4cf6-9773-897546c2d6bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'context_recall_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m eval_result \u001b[38;5;241m=\u001b[39m \u001b[43mcontext_recall_chain\u001b[49m(result)\n\u001b[0;32m      2\u001b[0m eval_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_recall_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'context_recall_chain' is not defined"
     ]
    }
   ],
   "source": [
    "eval_result = context_recall_chain(result)\n",
    "eval_result[\"context_recall_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118cba80-c2a3-408e-9d8d-857521cbe723",
   "metadata": {},
   "source": [
    "High context_recall_score means that the ground truth is present in the source documents.\n",
    "\n",
    "You can check lower context recall scores by changing the source_documents to something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6610c5f-5f8e-406c-885c-093012a5dc44",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[1;32m----> 2\u001b[0m fake_result \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      3\u001b[0m fake_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [Document(page_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love christmas\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m      4\u001b[0m eval_result \u001b[38;5;241m=\u001b[39m context_recall_chain(fake_result)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "fake_result = result.copy()\n",
    "fake_result[\"source_documents\"] = [Document(page_content=\"I love christmas\")]\n",
    "eval_result = context_recall_chain(fake_result)\n",
    "eval_result[\"context_recall_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc06403-c514-47fd-ac82-d95ece8d2f06",
   "metadata": {},
   "source": [
    "2. `evaluate()`\n",
    "\n",
    "Evaluate a list of inputs/queries and the outputs/predictions from the QA chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b23708-b94d-4649-acd9-0e268f72f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the queries as a batch for efficiency\n",
    "predictions = qa_chain.batch(examples)\n",
    "\n",
    "# evaluate\n",
    "print(\"evaluating...\")\n",
    "r = faithfulness_chain.evaluate(examples, predictions)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d661f6b9-04c7-40b7-874b-25f53cfab9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate context recall\n",
    "print(\"evaluating...\")\n",
    "r = context_recall_chain.evaluate(examples, predictions)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ddec885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ragas==0.0.11\n",
      "  Downloading ragas-0.0.11-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ragas==0.0.11) (1.26.4)\n",
      "Requirement already satisfied: transformers in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ragas==0.0.11) (4.46.1)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ragas==0.0.11) (3.2.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ragas==0.0.11) (3.0.2)\n",
      "Collecting protobuf<=3.20.0 (from ragas==0.0.11)\n",
      "  Downloading protobuf-3.20.0-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Requirement already satisfied: langchain>=0.0.218 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ragas==0.0.11) (0.3.7)\n",
      "Requirement already satisfied: openai in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ragas==0.0.11) (1.54.3)\n",
      "Collecting pydantic<2.0 (from ragas==0.0.11)\n",
      "  Downloading pydantic-1.10.19-cp312-cp312-win_amd64.whl.metadata (153 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain>=0.0.218->ragas==0.0.11) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain>=0.0.218->ragas==0.0.11) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain>=0.0.218->ragas==0.0.11) (3.10.8)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain>=0.0.218->ragas==0.0.11) (0.3.15)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain>=0.0.218->ragas==0.0.11) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain>=0.0.218->ragas==0.0.11) (0.1.139)\n",
      "INFO: pip is looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain>=0.0.218 (from ragas==0.0.11)\n",
      "  Downloading langchain-0.3.6-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.5-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain-0.3.4-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain-0.2.17-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.43 (from langchain>=0.0.218->ragas==0.0.11)\n",
      "  Downloading langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain>=0.0.218->ragas==0.0.11)\n",
      "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain>=0.0.218->ragas==0.0.11) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain>=0.0.218->ragas==0.0.11) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic<2.0->ragas==0.0.11) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets->ragas==0.0.11) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets->ragas==0.0.11) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets->ragas==0.0.11) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets->ragas==0.0.11) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets->ragas==0.0.11) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets->ragas==0.0.11) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets->ragas==0.0.11) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->ragas==0.0.11) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets->ragas==0.0.11) (0.24.7)\n",
      "Requirement already satisfied: packaging in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets->ragas==0.0.11) (24.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from openai->ragas==0.0.11) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from openai->ragas==0.0.11) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from openai->ragas==0.0.11) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from openai->ragas==0.0.11) (0.5.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from openai->ragas==0.0.11) (1.3.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers->ragas==0.0.11) (2.4.1+cu118)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers->ragas==0.0.11) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers->ragas==0.0.11) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers->ragas==0.0.11) (10.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers->ragas==0.0.11) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers->ragas==0.0.11) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers->ragas==0.0.11) (0.20.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->ragas==0.0.11) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->ragas==0.0.11) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->ragas==0.0.11) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->ragas==0.0.11) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->ragas==0.0.11) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->ragas==0.0.11) (1.13.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from anyio<5,>=3.5.0->openai->ragas==0.0.11) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1,>=0.23.0->openai->ragas==0.0.11) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1,>=0.23.0->openai->ragas==0.0.11) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->ragas==0.0.11) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain-core<0.3.0,>=0.2.43->langchain>=0.0.218->ragas==0.0.11) (1.33)\n",
      "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting openai (from ragas==0.0.11)\n",
      "  Using cached openai-1.54.3-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading openai-1.54.2-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading openai-1.54.1-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading openai-1.54.0-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading openai-1.53.1-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading openai-1.53.0-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading openai-1.52.2-py3-none-any.whl.metadata (24 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading openai-1.52.1-py3-none-any.whl.metadata (24 kB)\n",
      "  Using cached openai-1.52.0-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading openai-1.51.2-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading openai-1.51.1-py3-none-any.whl.metadata (24 kB)\n",
      "  Using cached openai-1.51.0-py3-none-any.whl.metadata (24 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading openai-1.50.2-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading openai-1.50.1-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading openai-1.50.0-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading openai-1.49.0-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading openai-1.48.0-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading openai-1.47.1-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading openai-1.47.0-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading openai-1.46.1-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading openai-1.46.0-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading openai-1.45.1-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.45.0-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.44.1-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.44.0-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.43.1-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.43.0-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.42.0-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.41.1-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.41.0-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.40.8-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.40.7-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.40.6-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.40.5-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.40.4-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.40.3-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.40.2-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.40.1-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.40.0-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.39.0-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.38.0-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.37.2-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.37.1-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.37.0-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.36.1-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.36.0-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.35.15-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading openai-1.35.14-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.35.13-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.35.12-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.35.11-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.35.10-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.35.9-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.35.8-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.35.7-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.35.6-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.35.5-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.35.4-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.35.3-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.35.2-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.35.1-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.35.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.34.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.33.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.32.1-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.32.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.31.2-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.31.1-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.31.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.30.5-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.30.4-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.30.3-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.30.2-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.30.1-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.30.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.29.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.28.2-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.28.1-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.28.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.27.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.26.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.25.2-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.25.1-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.25.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.24.1-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.24.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.23.6-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.23.5-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.23.4-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.23.3-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.23.2-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.23.1-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.23.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.22.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.21.2-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.21.1-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.20.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.19.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.18.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.17.1-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.17.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.16.2-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.16.1-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.16.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading openai-1.14.3-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading openai-1.14.2-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading openai-1.14.1-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading openai-1.14.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading openai-1.13.4-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading openai-1.13.3-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading openai-1.12.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading openai-1.11.1-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached openai-1.11.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading openai-1.10.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading openai-1.9.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading openai-1.8.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading openai-1.7.2-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.7.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.7.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.6.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.6.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.4.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.3.9-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.3.8-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.3.7-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting anyio<4,>=3.5.0 (from openai->ragas==0.0.11)\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting openai (from ragas==0.0.11)\n",
      "  Downloading openai-1.3.6-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.3.5-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.3.4-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.3.3-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.3.2-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.3.1-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.3.0-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.2.4-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.2.3-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.2.2-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.2.1-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.0.1-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting langchain>=0.0.218 (from ragas==0.0.11)\n",
      "  Downloading langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.38 (from langchain>=0.0.218->ragas==0.0.11)\n",
      "  Downloading langchain_core-0.2.42-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.41-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.40-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.39-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.38-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain>=0.0.218 (from ragas==0.0.11)\n",
      "  Downloading langchain-0.2.15-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.35 (from langchain>=0.0.218->ragas==0.0.11)\n",
      "  Downloading langchain_core-0.2.37-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.36-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.35-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain>=0.0.218 (from ragas==0.0.11)\n",
      "  Downloading langchain-0.2.14-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.32 (from langchain>=0.0.218->ragas==0.0.11)\n",
      "  Downloading langchain_core-0.2.34-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.33-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.32-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain>=0.0.218 (from ragas==0.0.11)\n",
      "  Downloading langchain-0.2.13-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.30 (from langchain>=0.0.218->ragas==0.0.11)\n",
      "  Downloading langchain_core-0.2.30-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain>=0.0.218 (from ragas==0.0.11)\n",
      "  Downloading langchain-0.2.12-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.27 (from langchain>=0.0.218->ragas==0.0.11)\n",
      "  Downloading langchain_core-0.2.29-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.28-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.27-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain>=0.0.218 (from ragas==0.0.11)\n",
      "  Downloading langchain-0.2.11-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.23 (from langchain>=0.0.218->ragas==0.0.11)\n",
      "  Downloading langchain_core-0.2.26-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.25-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.24-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.23-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain>=0.0.218 (from ragas==0.0.11)\n",
      "  Downloading langchain-0.2.10-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.22 (from langchain>=0.0.218->ragas==0.0.11)\n",
      "  Downloading langchain_core-0.2.22-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain>=0.0.218 (from ragas==0.0.11)\n",
      "  Downloading langchain-0.2.9-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.20 (from langchain>=0.0.218->ragas==0.0.11)\n",
      "  Downloading langchain_core-0.2.21-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.2.20-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain>=0.0.218 (from ragas==0.0.11)\n",
      "  Downloading langchain-0.2.8-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.19 (from langchain>=0.0.218->ragas==0.0.11)\n",
      "  Downloading langchain_core-0.2.19-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain>=0.0.218 (from ragas==0.0.11)\n",
      "  Downloading langchain-0.2.7-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.12 (from langchain>=0.0.218->ragas==0.0.11)\n",
      "  Downloading langchain_core-0.2.18-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.2.17-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.2.15-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.2.13-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.2.12-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain>=0.0.218 (from ragas==0.0.11)\n",
      "  Downloading langchain-0.2.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.10 (from langchain>=0.0.218->ragas==0.0.11)\n",
      "  Downloading langchain_core-0.2.11-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.2.10-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain>=0.0.218 (from ragas==0.0.11)\n",
      "  Downloading langchain-0.2.5-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.7 (from langchain>=0.0.218->ragas==0.0.11)\n",
      "  Downloading langchain_core-0.2.9-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.2.8-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain>=0.0.218->ragas==0.0.11)\n",
      "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain>=0.0.218->ragas==0.0.11) (3.10.7)\n",
      "INFO: pip is looking at multiple versions of langsmith to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain>=0.0.218->ragas==0.0.11)\n",
      "  Downloading langsmith-0.1.142-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.138-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.137-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.134-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.133-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.132-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.131-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of langsmith to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langsmith-0.1.129-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.128-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.127-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.126-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.125-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading langsmith-0.1.124-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.123-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.122-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.121-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.120-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.119-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.118-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.117-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.116-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.115-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.114-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.113-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.112-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.111-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.110-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.109-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.108-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.107-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.106-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.105-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.104-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.103-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.102-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.101-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.99-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.98-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.97-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.96-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.95-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.94-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.93-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.92-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.91-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.90-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.89-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.88-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.87-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.86-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.85-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.84-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.83-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.82-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.81-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2->langchain>=0.0.218->ragas==0.0.11) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2->langchain>=0.0.218->ragas==0.0.11) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.218->ragas==0.0.11) (3.1.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers->ragas==0.0.11) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers->ragas==0.0.11) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers->ragas==0.0.11) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers->ragas==0.0.11) (75.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm>=4.66.3->datasets->ragas==0.0.11) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->datasets->ragas==0.0.11) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->datasets->ragas==0.0.11) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->datasets->ragas==0.0.11) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn->sentence-transformers->ragas==0.0.11) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn->sentence-transformers->ragas==0.0.11) (3.5.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.43->langchain>=0.0.218->ragas==0.0.11) (3.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas==0.0.11) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers->ragas==0.0.11) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mktmi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy->torch>=1.11.0->sentence-transformers->ragas==0.0.11) (1.3.0)\n",
      "Downloading ragas-0.0.11-py3-none-any.whl (31 kB)\n",
      "Downloading langchain-0.2.5-py3-none-any.whl (974 kB)\n",
      "   ---------------------------------------- 0.0/974.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/974.6 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 262.1/974.6 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 786.4/974.6 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 974.6/974.6 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading pydantic-1.10.19-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 645.7 kB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 838.9 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.0/2.0 MB 931.8 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/2.0 MB 986.4 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/2.0 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.2.8-py3-none-any.whl (315 kB)\n",
      "Downloading protobuf-3.20.0-py2.py3-none-any.whl (162 kB)\n",
      "Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
      "Downloading langsmith-0.1.81-py3-none-any.whl (127 kB)\n",
      "Installing collected packages: pydantic, protobuf, langsmith, langchain-core, langchain-text-splitters, langchain, ragas\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.9.2\n",
      "    Uninstalling pydantic-2.9.2:\n",
      "      Successfully uninstalled pydantic-2.9.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.4\n",
      "    Uninstalling protobuf-4.25.4:\n",
      "      Successfully uninstalled protobuf-4.25.4\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.139\n",
      "    Uninstalling langsmith-0.1.139:\n",
      "      Successfully uninstalled langsmith-0.1.139\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.15\n",
      "    Uninstalling langchain-core-0.3.15:\n",
      "      Successfully uninstalled langchain-core-0.3.15\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.3.0\n",
      "    Uninstalling langchain-text-splitters-0.3.0:\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.0\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.7\n",
      "    Uninstalling langchain-0.3.7:\n",
      "      Successfully uninstalled langchain-0.3.7\n",
      "  Attempting uninstall: ragas\n",
      "    Found existing installation: ragas 0.2.4\n",
      "    Uninstalling ragas-0.2.4:\n",
      "      Successfully uninstalled ragas-0.2.4\n",
      "Successfully installed langchain-0.2.5 langchain-core-0.2.8 langchain-text-splitters-0.2.1 langsmith-0.1.81 protobuf-3.20.0 pydantic-1.10.19 ragas-0.0.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\mktmi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\~upb'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "googleapis-common-protos 1.65.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
      "google-api-core 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
      "gradio 5.4.0 requires huggingface-hub>=0.25.1, but you have huggingface-hub 0.24.7 which is incompatible.\n",
      "gradio 5.4.0 requires pydantic>=2.0, but you have pydantic 1.10.19 which is incompatible.\n",
      "langchain-community 0.3.1 requires langchain<0.4.0,>=0.3.1, but you have langchain 0.2.5 which is incompatible.\n",
      "langchain-community 0.3.1 requires langchain-core<0.4.0,>=0.3.6, but you have langchain-core 0.2.8 which is incompatible.\n",
      "langchain-community 0.3.1 requires langsmith<0.2.0,>=0.1.125, but you have langsmith 0.1.81 which is incompatible.\n",
      "langchain-elasticsearch 0.3.0 requires langchain-core<0.4.0,>=0.3.0, but you have langchain-core 0.2.8 which is incompatible.\n",
      "langchain-experimental 0.3.2 requires langchain-core<0.4.0,>=0.3.6, but you have langchain-core 0.2.8 which is incompatible.\n",
      "langchain-ollama 0.2.0 requires langchain-core<0.4.0,>=0.3.0, but you have langchain-core 0.2.8 which is incompatible.\n",
      "langchain-openai 0.2.3 requires langchain-core<0.4.0,>=0.3.12, but you have langchain-core 0.2.8 which is incompatible.\n",
      "langserve 0.3.0 requires langchain-core<0.4,>=0.3, but you have langchain-core 0.2.8 which is incompatible.\n",
      "langserve 0.3.0 requires pydantic<3.0,>=2.7, but you have pydantic 1.10.19 which is incompatible.\n",
      "llama-index-core 0.11.22 requires pydantic<3.0.0,>=2.7.0, but you have pydantic 1.10.19 which is incompatible.\n",
      "pydantic-settings 2.5.2 requires pydantic>=2.7.0, but you have pydantic 1.10.19 which is incompatible.\n",
      "tensorflow-intel 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install ragas==0.0.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c659e18",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'EvaluationMode' from 'ragas.metrics.base' (C:\\Users\\mktmi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ragas\\metrics\\base.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevalchain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RagasEvaluatorChain \u001b[38;5;66;03m# Importing RagasEvaluatorChain\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RetrievalQA\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the RagasEvaluatorChain\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ragas\\langchain\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevalchain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RagasEvaluatorChain\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ragas\\langchain\\evalchain.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangsmith\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationResult, RunEvaluator\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangsmith\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschemas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Example, Run\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationMode, Metric\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EVALMODE_TO_COLUMNS\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mTYPE_CHECKING:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'EvaluationMode' from 'ragas.metrics.base' (C:\\Users\\mktmi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ragas\\metrics\\base.py)"
     ]
    }
   ],
   "source": [
    "from ragas.langchain.evalchain import RagasEvaluatorChain # Importing RagasEvaluatorChain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Initialize the RagasEvaluatorChain\n",
    "evaluator = RagasEvaluatorChain()\n",
    "\n",
    "# Run a single QA example to evaluate with `__call__()`\n",
    "# Using an example question from eval_questions for testing\n",
    "result = qa_chain.invoke({\"query\": eval_questions[1]})\n",
    "print(result[\"result\"])  # Display result for confirmation\n",
    "\n",
    "# Key mapping to make result more readable\n",
    "key_mapping = {\n",
    "    \"query\": \"question\",\n",
    "    \"result\": \"answer\",\n",
    "    \"source_documents\": \"contexts\"\n",
    "}\n",
    "\n",
    "# Create a new dictionary with updated key names\n",
    "result_updated = {}\n",
    "for old_key, new_key in key_mapping.items():\n",
    "    if old_key in result:\n",
    "        result_updated[new_key] = result[old_key]\n",
    "\n",
    "# Display the updated result structure\n",
    "print(result_updated)\n",
    "\n",
    "# To evaluate on multiple examples, use `evaluate()`\n",
    "# Collecting predictions by invoking qa_chain on each question in eval_questions\n",
    "predictions = [qa_chain.invoke({\"query\": q})[\"result\"] for q in eval_questions]\n",
    "\n",
    "# Evaluation setup with ground truths\n",
    "evaluation_results = evaluator.evaluate(\n",
    "    examples=[{\"query\": q, \"ground_truths\": [a]} for q, a in zip(eval_questions, eval_answers)],\n",
    "    predictions=predictions\n",
    ")\n",
    "\n",
    "# Display evaluation results for batch evaluation\n",
    "print(evaluation_results)\n",
    "\n",
    "# If working with LangSmith datasets, you can use `evaluate_run()`\n",
    "# evaluator.evaluate_run(dataset=your_langsmith_dataset)  # Uncomment if LangSmith dataset is available\n",
    "\n",
    "# Explanation of Faithfulness and Context Relevancy Metrics\n",
    "# High faithfulness_score: Indicates consistency between source documents and the answer.\n",
    "# To see lower faithfulness scores, try altering the `result` or `source_documents`.\n",
    "\n",
    "# High context_recall_score: Shows that the ground truth is well-represented in the source documents.\n",
    "# Adjust the source_documents to something else for lower context recall scores.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paperspace Env",
   "language": "python",
   "name": "paperspace-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
